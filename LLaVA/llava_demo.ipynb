{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Демонстрация LLaVA: Крупная языковая и визуальная модель\n",
    "\n",
    "Этот ноутбук демонстрирует работу модели LLaVA (Large Language and Vision Assistant), представленной на конференции NeurIPS 2023. LLaVA объединяет возможности обработки изображений и текста, используя архитектуру, состоящую из визуального энкодера CLIP, проекционного слоя и языковой модели Vicuna.\n",
    "\n",
    "## Основные компоненты LLaVA:\n",
    "1. **Визуальный энкодер CLIP-ViT-L/14** для преобразования изображений в векторные представления\n",
    "2. **Проекционный слой**, отображающий визуальные эмбеддинги в пространство языковой модели\n",
    "3. **Языковая модель Vicuna** (на базе LLaMA) с 13B параметров, отвечающая за генерацию текста\n",
    "\n",
    "Давайте установим необходимые зависимости и загрузим модель для демонстрации её возможностей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Установка необходимых библиотек\n",
    "\n",
    "Сначала установим необходимые зависимости для работы с LLaVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install torch torchvision transformers accelerate sentencepiece protobuf==3.20.3 gradio\n",
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Клонирование репозитория LLaVA\n",
    "\n",
    "Клонируем официальный репозиторий LLaVA для использования его функциональности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!git clone https://github.com/haotian-liu/LLaVA.git\n",
    "%cd LLaVA\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Загрузка модели LLaVA\n",
    "\n",
    "Загрузим предобученную модель LLaVA с Hugging Face. Мы будем использовать версию LLaVA-1.5, которая является улучшенной версией оригинальной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "from llava.conversation import conv_templates\n",
    "from llava.utils import disable_torch_init\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Отключаем инициализацию torch для ускорения загрузки\n",
    "disable_torch_init()\n",
    "\n",
    "# Загружаем модель LLaVA-1.5 7B (меньшая версия для быстрой работы в Colab)\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "print(\"Модель LLaVA успешно загружена!\")\n",
    "print(f\"Имя модели: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Функция для обработки изображений и генерации ответов\n",
    "\n",
    "Создадим функцию, которая будет принимать изображение и вопрос, а затем генерировать ответ с помощью модели LLaVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_image_and_generate_response(image, prompt, temperature=0.2, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    Обрабатывает изображение и генерирует текстовый ответ на основе заданного вопроса.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL изображение или URL изображения\n",
    "        prompt: Текстовый вопрос или инструкция\n",
    "        temperature: Параметр температуры для генерации текста (0.0-1.0)\n",
    "        max_new_tokens: Максимальное количество новых токенов для генерации\n",
    "        \n",
    "    Returns:\n",
    "        Текстовый ответ модели\n",
    "    \"\"\"\n",
    "    # Если передан URL, загружаем изображение\n",
    "    if isinstance(image, str) and (image.startswith('http://') or image.startswith('https://')):\n",
    "        response = requests.get(image)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    elif not isinstance(image, Image.Image):\n",
    "        image = Image.open(image).convert('RGB')\n",
    "    \n",
    "    # Подготавливаем шаблон разговора\n",
    "    # Исправление: используем 'vicuna' вместо model_name в качестве ключа\n",
    "    conv = conv_templates['vicuna'].copy()\n",
    "    conv.append_message(conv.roles[0], prompt)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    \n",
    "    # Обрабатываем изображение и генерируем ответ\n",
    "    image_tensor = processor(image).unsqueeze(0).to(model.device)\n",
    "    \n",
    "    input_ids = tokenizer(prompt).input_ids\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(model.device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=image_tensor,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "    \n",
    "    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Создание интерактивного интерфейса с помощью Gradio\n",
    "\n",
    "Создадим простой веб-интерфейс для взаимодействия с моделью LLaVA с помощью библиотеки Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import gradio as gr\n",
    "\n",
    "def llava_interface(image, prompt, temperature=0.2, max_tokens=512):\n",
    "    if image is None:\n",
    "        return \"Пожалуйста, загрузите изображение.\"\n",
    "    if not prompt:\n",
    "        return \"Пожалуйста, введите вопрос или инструкцию.\"\n",
    "    \n",
    "    try:\n",
    "        response = process_image_and_generate_response(\n",
    "            image, prompt, temperature=temperature, max_new_tokens=max_tokens\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Произошла ошибка: {str(e)}\"\n",
    "\n",
    "# Создаем интерфейс\n",
    "demo = gr.Interface(\n",
    "    fn=llava_interface,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Загрузите изображение\"),\n",
    "        gr.Textbox(lines=2, placeholder=\"Введите вопрос или инструкцию...\", label=\"Вопрос/Инструкция\"),\n",
    "        gr.Slider(minimum=0.0, maximum=1.0, value=0.2, step=0.1, label=\"Температура\"),\n",
    "        gr.Slider(minimum=64, maximum=1024, value=512, step=64, label=\"Максимальное количество токенов\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Ответ модели\"),\n",
    "    title=\"Демонстрация LLaVA: Крупная языковая и визуальная модель\",\n",
    "    description=\"Загрузите изображение и задайте вопрос или инструкцию. Модель LLaVA проанализирует изображение и сгенерирует текстовый ответ.\"\n",
    ")\n",
    "\n",
    "# Запускаем интерфейс\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Примеры использования LLaVA\n",
    "\n",
    "Давайте рассмотрим несколько примеров использования модели LLaVA с различными типами изображений и запросов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Пример 1: Описание изображения\n",
    "image_url = \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\"\n",
    "response = requests.get(image_url)\n",
    "example_image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "example_image.save('example_image.jpg')\n",
    "example_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Запрос на описание изображения\n",
    "prompt = \"Опиши подробно, что ты видишь на этом изображении.\"\n",
    "response = process_image_and_generate_response(example_image, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Пример 2: Пространственное рассуждение\n",
    "prompt = \"Что находится слева от человека на изображении? Опиши этот объект.\"\n",
    "response = process_image_and_generate_response(example_image, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Пример 3: Сложное рассуждение\n",
    "prompt = \"Предположи, какое время года изображено на фотографии и объясни, почему ты так считаешь.\"\n",
    "response = process_image_and_generate_response(example_image, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Загрузка собственного изображения\n",
    "\n",
    "Вы можете загрузить собственное изображение и задать вопрос или инструкцию. Для этого используйте интерактивный интерфейс, созданный выше, или следующий код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from google.colab import files\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Загрузка изображения\n",
    "uploaded = files.upload()\n",
    "image_path = list(uploaded.keys())[0]\n",
    "user_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Отображение изображения\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(user_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Ввод вопроса\n",
    "user_prompt = input(\"Введите вопрос или инструкцию: \")\n",
    "\n",
    "# Генерация ответа\n",
    "response = process_image_and_generate_response(user_image, user_prompt)\n",
    "print(\"\\nОтвет модели:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Заключение\n",
    "\n",
    "В этом ноутбуке мы продемонстрировали работу модели LLaVA, которая объединяет возможности обработки изображений и текста. Модель способна:\n",
    "\n",
    "1. Детально описывать содержимое изображений\n",
    "2. Отвечать на вопросы о пространственных отношениях объектов\n",
    "3. Проводить сложные рассуждения на основе визуальной информации\n",
    "4. Следовать инструкциям пользователя при анализе изображений\n",
    "\n",
    "LLaVA представляет собой значительный шаг в создании универсальных мультимодальных ассистентов, способных понимать как текстовую, так и визуальную информацию.\n",
    "\n",
    "### Ссылки и ресурсы\n",
    "\n",
    "- [Официальный репозиторий LLaVA на GitHub](https://github.com/haotian-liu/LLaVA)\n",
    "- [Статья LLaVA на NeurIPS 2023](https://arxiv.org/abs/2304.08485)\n",
    "- [Модели LLaVA на Hugging Face](https://huggingface.co/liuhaotian/llava-v1.5-7b)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
