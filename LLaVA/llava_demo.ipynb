{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaVA Demo: Large Language and Vision Model\n",
    "\n",
    "This notebook demonstrates the LLaVA (Large Language and Vision Assistant) model, presented at the NeurIPS 2023 conference. LLaVA combines image and text processing capabilities using an architecture consisting of a CLIP visual encoder, a projection layer, and the Vicuna language model.\n",
    "\n",
    "## Key Components of LLaVA:\n",
    "1. **CLIP-ViT-L/14 Visual Encoder** for converting images into vector representations\n",
    "2. **Projection Layer** mapping visual embeddings into the language model's space\n",
    "3. **Vicuna Language Model** (based on LLaMA) with 13B parameters, responsible for text generation\n",
    "\n",
    "Let's install the necessary dependencies and load the model to demonstrate its capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing Required Libraries\n",
    "\n",
    "First, let's install the necessary dependencies to work with LLaVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers accelerate sentencepiece protobuf==3.20.3 gradio\n",
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cloning the LLaVA Repository\n",
    "\n",
    "Clone the official LLaVA repository to use its functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/haotian-liu/LLaVA.git\n",
    "%cd LLaVA\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the LLaVA Model\n",
    "\n",
    "Let's load the pre-trained LLaVA model from Hugging Face. We will use the LLaVA-1.5 version, which is an improved version of the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "from llava.conversation import conv_templates\n",
    "from llava.utils import disable_torch_init\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "disable_torch_init()\n",
    "\n",
    "# Load the LLaVA-1.5 7B model (smaller version for quick use in Colab)\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "print(\"LLaVA model successfully loaded!\")\n",
    "print(f\"Model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Function for Image Processing and Response Generation\n",
    "\n",
    "Let's create a function that will take an image and a question, and then generate a response using the LLaVA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_and_generate_response(image, prompt, temperature=0.2, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    Processes an image and generates a text response based on the given question.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL image or image URL\n",
    "        prompt: Text question or instruction\n",
    "        temperature: Temperature parameter for text generation (0.0-1.0)\n",
    "        max_new_tokens: Maximum number of new tokens for generation\n",
    "        \n",
    "    Returns:\n",
    "        Text response from the model\n",
    "    \"\"\"\n",
    "    if isinstance(image, str) and (image.startswith('http://') or image.startswith('https://')):\n",
    "        response = requests.get(image)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    elif not isinstance(image, Image.Image):\n",
    "        image = Image.open(image).convert('RGB')\n",
    "    \n",
    "    conv = conv_templates['vicuna'].copy()\n",
    "    conv.append_message(conv.roles[0], prompt)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    \n",
    "    image_tensor = processor(image).unsqueeze(0).to(model.device)\n",
    "    \n",
    "    input_ids = tokenizer(prompt).input_ids\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(model.device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=image_tensor,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "    \n",
    "    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating an Interactive Interface with Gradio\n",
    "\n",
    "Let's create a simple web interface to interact with the LLaVA model using the Gradio library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def llava_interface(image, prompt, temperature=0.2, max_tokens=512):\n",
    "    if image is None:\n",
    "        return \"Please upload an image.\"\n",
    "    if not prompt:\n",
    "        return \"Please enter a question or instruction.\"\n",
    "    \n",
    "    try:\n",
    "        response = process_image_and_generate_response(\n",
    "            image, prompt, temperature=temperature, max_new_tokens=max_tokens\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=llava_interface,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Upload an Image\"),\n",
    "        gr.Textbox(lines=2, placeholder=\"Enter a question or instruction...\", label=\"Question/Instruction\"),\n",
    "        gr.Slider(minimum=0.0, maximum=1.0, value=0.2, step=0.1, label=\"Temperature\"),\n",
    "        gr.Slider(minimum=64, maximum=1024, value=512, step=64, label=\"Maximum Number of Tokens\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Model Response\"),\n",
    "    title=\"LLaVA Demo: Large Language and Vision Model\",\n",
    "    description=\"Upload an image and ask a question or provide an instruction. The LLaVA model will analyze the image and generate a text response.\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LLaVA Usage Examples\n",
    "\n",
    "Let's look at a few examples of using the LLaVA model with different types of images and prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\"\n",
    "response = requests.get(image_url)\n",
    "example_image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "example_image.save('example_image.jpg')\n",
    "example_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Describe in detail what you see in this image.\"\n",
    "response = process_image_and_generate_response(example_image, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is to the left of the person in the image? Describe this object.\"\n",
    "response = process_image_and_generate_response(example_image, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Guess the season depicted in the photo and explain why you think so.\"\n",
    "response = process_image_and_generate_response(example_image, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Uploading Your Own Image\n",
    "\n",
    "You can upload your own image and ask a question or provide an instruction. Use the interactive interface created above or the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "uploaded = files.upload()\n",
    "image_path = list(uploaded.keys())[0]\n",
    "user_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(user_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "user_prompt = input(\"Enter a question or instruction: \")\n",
    "\n",
    "response = process_image_and_generate_response(user_image, user_prompt)\n",
    "print(\"\\nModel Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we demonstrated the LLaVA model, which combines image and text processing capabilities. The model is capable of:\n",
    "\n",
    "1. Describing image content in detail\n",
    "2. Answering questions about the spatial relationships of objects\n",
    "3. Performing complex reasoning based on visual information\n",
    "4. Following user instructions when analyzing images\n",
    "\n",
    "LLaVA represents a significant step in creating versatile multimodal assistants capable of understanding both textual and visual information.\n",
    "\n",
    "### Links and Resources\n",
    "\n",
    "- [Official LLaVA Repository on GitHub](https://github.com/haotian-liu/LLaVA)\n",
    "- [LLaVA Paper at NeurIPS 2023](https://arxiv.org/abs/2304.08485)\n",
    "- [LLaVA Models on Hugging Face](https://huggingface.co/liuhaotian/llava-v1.5-7b)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
