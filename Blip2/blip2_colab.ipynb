{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing BLIP-2 Results in Google Colab\n",
    "\n",
    "This notebook runs the BLIP-2 (Bootstrap Language-Image Pretraining with Frozen Image Encoders and Large Language Models) model described in the paper \"BLIP-2: Bootstrap Image and Language Pretraining with Frozen Image Encoders and Large Language Models\".\n",
    "\n",
    "BLIP-2 is an efficient method for training multimodal models that combine computer vision and natural language. The main innovation is the use of pretrained frozen models for both modalities, which reduces computational cost and improves performance.\n",
    "\n",
    "The key component is a lightweight Querying Transformer (Q-Former) trained with a two-stage strategy to bridge the gap between modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/salesforce/LAVIS.git\n",
    "%cd LAVIS\n",
    "\n",
    "!pip install -e .\n",
    "!pip install transformers==4.28.0\n",
    "!pip install accelerate\n",
    "!pip install fairscale\n",
    "!pip install timm\n",
    "!pip install pycocoevalcap\n",
    "!pip install opencv-python==4.10.0.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Initializing the BLIP-2 Model\n",
    "\n",
    "Now let's load the pre-trained BLIP-2 model. The LAVIS library provides a convenient interface for working with various models, including BLIP-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from lavis.models import load_model_and_preprocess\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vis_processors, _ = load_model_and_preprocess(\n",
    "    name=\"blip2_opt\", \n",
    "    model_type=\"pretrain_opt2.7b\", \n",
    "    is_eval=True, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading and processing the image\n",
    "\n",
    "Let's load a test image to demonstrate how the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url):\n",
    "    raw_image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "    return raw_image\n",
    "\n",
    "image_url = \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\"\n",
    "raw_image = load_image_from_url(image_url)\n",
    "display(raw_image)\n",
    "\n",
    "image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Demonstration of BLIP-2 capabilities\n",
    "\n",
    "### 4.1 Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = model.generate({\"image\": image})\n",
    "print(f\"Generated caption: {caption[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visual Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(model, image, question):\n",
    "    answer = model.generate({\"image\": image, \"prompt\": f\"Question: {question} Answer:\"})\n",
    "    return answer[0]\n",
    "\n",
    "questions = [\n",
    "    \"What is shown in the photo?\",\n",
    "    \"What color is the bus?\",\n",
    "    \"How many people are visible in the image?\",\n",
    "    \"What is the weather like in the image?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = answer_question(model, image, question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Instruction-based Image-to-Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_instruction(model, image, instruction):\n",
    "    generated_text = model.generate({\"image\": image, \"prompt\": instruction})\n",
    "    return generated_text[0]\n",
    "\n",
    "instructions = [\n",
    "    \"Describe in detail what is happening in the image.\",\n",
    "    \"Write a short story based on this image.\",\n",
    "    \"List all the objects you can see in the image.\",\n",
    "    \"Explain what emotions this image evokes and why.\"\n",
    "]\n",
    "\n",
    "for instruction in instructions:\n",
    "    generated_text = generate_text_with_instruction(model, image, instruction)\n",
    "    print(f\"Instruction: {instruction}\")\n",
    "    print(f\"Generated text: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loading BLIP-2 with other language models\n",
    "\n",
    "BLIP-2 can use different language models. Let's try loading BLIP-2 with T5 as the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t5, vis_processors_t5, _ = load_model_and_preprocess(\n",
    "    name=\"blip2_t5\", \n",
    "    model_type=\"pretrain_flant5xl\", \n",
    "    is_eval=True, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_t5 = model_t5.generate({\"image\": image})\n",
    "print(f\"Generated caption (T5): {caption_t5[0]}\")\n",
    "\n",
    "question = \"What is shown in the photo?\"\n",
    "answer_t5 = model_t5.generate({\"image\": image, \"prompt\": f\"Question: {question} Answer:\"})\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer (T5): {answer_t5[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Q-Former Architecture Analysis\n",
    "\n",
    "Let's look at the architecture of Q-Former, which is a key component of BLIP-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "print(\"\\nBLIP-2 model structure:\")\n",
    "for name, module in model.named_children():\n",
    "    print(f\"- {name}: {type(module).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing on custom images\n",
    "\n",
    "Let's test the model on custom images. You can upload your own image or use a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "def load_image_from_upload():\n",
    "    uploaded = files.upload()\n",
    "    for filename in uploaded.keys():\n",
    "        print(f\"Uploaded image: {filename}\")\n",
    "        raw_image = Image.open(filename).convert(\"RGB\")\n",
    "        return raw_image, filename\n",
    "\n",
    "try:\n",
    "    user_image, filename = load_image_from_upload()\n",
    "    display(user_image)\n",
    "    \n",
    "    processed_image = vis_processors[\"eval\"](user_image).unsqueeze(0).to(device)\n",
    "    \n",
    "    user_caption = model.generate({\"image\": processed_image})\n",
    "    print(f\"Generated caption: {user_caption[0]}\")\n",
    "    \n",
    "    user_question = \"What is shown in this photo?\"\n",
    "    user_answer = model.generate({\"image\": processed_image, \"prompt\": f\"Question: {user_question} Answer:\"})\n",
    "    print(f\"Question: {user_question}\")\n",
    "    print(f\"Answer: {user_answer[0]}\")\n",
    "except:\n",
    "    print(\"The image was not uploaded or an error occurred during processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Uploading an image by URL\n",
    "\n",
    "Alternatively, you can use an image by URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "url_input = widgets.Text(\n",
    "    value='https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg',\n",
    "    placeholder='Enter image URL',\n",
    "    description='URL:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "display(url_input)\n",
    "\n",
    "def process_url_image(url):\n",
    "    try:\n",
    "        url_image = load_image_from_url(url)\n",
    "        display(url_image)\n",
    "        \n",
    "        processed_url_image = vis_processors[\"eval\"](url_image).unsqueeze(0).to(device)\n",
    "        \n",
    "        url_caption = model.generate({\"image\": processed_url_image})\n",
    "        print(f\"Generated caption: {url_caption[0]}\")\n",
    "        \n",
    "        url_question = \"What is shown in this photo?\"\n",
    "        url_answer = model.generate({\"image\": processed_url_image, \"prompt\": f\"Question: {url_question} Answer:\"})\n",
    "        print(f\"Question: {url_question}\")\n",
    "        print(f\"Answer: {url_answer[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing the image: {e}\")\n",
    "\n",
    "process_button = widgets.Button(description=\"Process Image\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        process_url_image(url_input.value)\n",
    "\n",
    "process_button.on_click(on_button_clicked)\n",
    "\n",
    "display(process_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we successfully reproduced the results of BLIP-2, demonstrating its capabilities in various image-text interaction tasks:\n",
    "\n",
    "1. Image caption generation\n",
    "2. Image question answering\n",
    "3. Text generation from an image with instructions\n",
    "\n",
    "BLIP-2 is an efficient method for pre-training multimodal models that uses frozen pre-trained models for both modalities, which significantly reduces the computational cost and improves the performance.\n",
    "\n",
    "The key component is a lightweight Querying Transformer (Q-Former), trained with a two-stage strategy to bridge the gap between modalities, allowing the model to achieve high results with significantly fewer training parameters compared to existing methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
