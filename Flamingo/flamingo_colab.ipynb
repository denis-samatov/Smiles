{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "# Reproducing the Open-Flamingo Model\n",
        "\n",
        "This notebook demonstrates the functionality of Open-Flamingo - an open implementation of DeepMind's Flamingo model for multimodal few-shot learning.\n",
        "\n",
        "## What is Flamingo?\n",
        "\n",
        "Flamingo is a multimodal model developed by DeepMind that can process both images and text. A key feature of the model is its ability for few-shot learning, meaning it can learn from a small number of examples without additional fine-tuning for a specific task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install open-flamingo\n",
        "!pip install torch==2.0.1\n",
        "!pip install transformers==4.33.0\n",
        "!pip install pillow\n",
        "!pip install matplotlib\n",
        "!pip install huggingface_hub\n",
        "!pip install numpy==1.26.4\n",
        "!pip install triton_pre_mlir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from huggingface_hub import hf_hub_download\n",
        "from open_flamingo import create_model_and_transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the Model\n",
        "\n",
        "Load the pre-trained Open-Flamingo model. In this example, we will use the model based on CLIP ViT-L/14 and MPT-7B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model, image_processor, tokenizer = create_model_and_transforms(\n",
        "    clip_vision_encoder_path=\"ViT-L-14\",\n",
        "    clip_vision_encoder_pretrained=\"openai\",\n",
        "    lang_encoder_path=\"mosaicml/mpt-7b\",\n",
        "    tokenizer_path=\"mosaicml/mpt-7b\",\n",
        "    cross_attn_every_n_layers=4\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functions for Loading and Processing Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    return Image.open(BytesIO(response.content))\n",
        "\n",
        "def display_image(image):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def process_image(image):\n",
        "    processed_image = image_processor(image).unsqueeze(0).to(device)\n",
        "    return processed_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Few-Shot Learning Demonstration\n",
        "\n",
        "Now let's demonstrate the few-shot learning capabilities of the Open-Flamingo model using the image captioning task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_image_urls = [\n",
        "    \"https://www.jupiter.fl.us/ImageRepository/Document?documentID=28619\",  # Dog on the beach\n",
        "    \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQrDDytaLshWVs9WWU1Mfc0S8ySfY5_q2IKYA&s\"   # Cat on the couch\n",
        "]\n",
        "\n",
        "example_images = [load_image_from_url(url) for url in example_image_urls]\n",
        "processed_example_images = [process_image(img) for img in example_images]\n",
        "\n",
        "for i, img in enumerate(example_images):\n",
        "    print(f\"Example {i+1}:\")\n",
        "    display_image(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_texts = [\n",
        "    \"<image>This image shows a dog running on the beach.\",\n",
        "    \"<image>This image shows a cat lying on the couch.\"\n",
        "]\n",
        "\n",
        "test_image_url = \"https://images.unsplash.com/photo-1527444803827-9503bda211a0?fm=jpg&q=60&w=3000&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8NHx8YmlyZCUyMG9uJTIwYSUyMGJyYW5jaHxlbnwwfHwwfHx8MA%3D%3D\"  # Bird on a branch\n",
        "test_image = load_image_from_url(test_image_url)\n",
        "processed_test_image = process_image(test_image)\n",
        "\n",
        "print(\"Test Image:\")\n",
        "display_image(test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_caption(model, tokenizer, example_images, example_texts, test_image, max_length=50):\n",
        "    prompt = \"\"\n",
        "    for text in example_texts:\n",
        "        prompt += text + \"\\n\"\n",
        "    \n",
        "    prompt += \"<image>\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    all_images = processed_example_images + [processed_test_image]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            vision_x=torch.cat(all_images),\n",
        "            lang_x=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=max_length,\n",
        "            num_beams=3,\n",
        "            temperature=0.7\n",
        "        )\n",
        "    \n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    \n",
        "    generated_caption = generated_text.split(\"<image>\")[-1].strip()\n",
        "    \n",
        "    return generated_caption\n",
        "\n",
        "caption = generate_caption(model, tokenizer, processed_example_images, example_texts, processed_test_image)\n",
        "print(\"Generated Caption:\")\n",
        "print(caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visual Question Answering (VQA) Demonstration\n",
        "\n",
        "Now let's demonstrate the model's capabilities for answering questions about images (VQA)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vqa_example_image_urls = [\n",
        "    \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQmdUuX6S9wL8mqeBEN7govjy_e8k3hr5Pi5w&s\",  # Red car\n",
        "    \"https://media.istockphoto.com/id/1311353295/photo/handsome-african-american-man-in-the-city-on-a-rainy-day.jpg?s=612x612&w=0&k=20&c=4IMDl0fUa2q8_H73wPm6RsdTFmXvs6SpQlsYzA-3yV0=\"   # Person with an umbrella\n",
        "]\n",
        "\n",
        "vqa_example_images = [load_image_from_url(url) for url in vqa_example_image_urls]\n",
        "processed_vqa_example_images = [process_image(img) for img in vqa_example_images]\n",
        "\n",
        "# Display examples\n",
        "for i, img in enumerate(vqa_example_images):\n",
        "    print(f\"VQA Example {i+1}:\")\n",
        "    display_image(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vqa_example_texts = [\n",
        "    \"<image>Question: What color is the car? Answer: Red.\",\n",
        "    \"<image>Question: What is the person holding? Answer: An umbrella.\"\n",
        "]\n",
        "\n",
        "vqa_test_image_url = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQTRaNUD8fzxaV1Alela2SiKVJhxL88M4AJ4g&s\"  # Bicycle\n",
        "vqa_test_image = load_image_from_url(vqa_test_image_url)\n",
        "processed_vqa_test_image = process_image(vqa_test_image)\n",
        "\n",
        "print(\"Test Image for VQA:\")\n",
        "display_image(vqa_test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer_question(model, tokenizer, example_images, example_texts, test_image, question, max_length=30):\n",
        "    prompt = \"\"\n",
        "    for text in example_texts:\n",
        "        prompt += text + \"\\n\"\n",
        "    \n",
        "    prompt += f\"<image>Question: {question} Answer:\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    all_images = processed_vqa_example_images + [processed_vqa_test_image]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            vision_x=torch.cat(all_images),\n",
        "            lang_x=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=max_length,\n",
        "            num_beams=3,\n",
        "            temperature=0.7\n",
        "        )\n",
        "    \n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    \n",
        "    answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "    \n",
        "    return answer\n",
        "\n",
        "question = \"How many wheels does the vehicle in the image have?\"\n",
        "answer = answer_question(model, tokenizer, processed_vqa_example_images, vqa_example_texts, processed_vqa_test_image, question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments with Different Numbers of Examples (Shots)\n",
        "\n",
        "Let's demonstrate how the quality of the model's answers changes with the number of examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "additional_image_urls = [\n",
        "    \"https://t3.ftcdn.net/jpg/00/20/13/60/360_F_20136083_gk0ppzak6UdK9PcDRgPdLjcuAdo7o1LK.jpg\",  # Airplane\n",
        "    \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR0uPhkGttm-ih7VQrplJGzhRyFdD938FVirg&s\"   # Ship\n",
        "]\n",
        "\n",
        "additional_images = [load_image_from_url(url) for url in additional_image_urls]\n",
        "processed_additional_images = [process_image(img) for img in additional_images]\n",
        "\n",
        "for i, img in enumerate(additional_images):\n",
        "    print(f\"Additional Example {i+1}:\")\n",
        "    display_image(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "additional_texts = [\n",
        "    \"<image>This image shows an airplane flying in the sky.\",\n",
        "    \"<image>This image shows a ship sailing on the sea.\"\n",
        "]\n",
        "\n",
        "print(\"Results with 0 examples (zero-shot):\")\n",
        "zero_shot_caption = generate_caption(model, tokenizer, [], [], processed_test_image)\n",
        "print(zero_shot_caption)\n",
        "print(\"\\nResults with 2 examples (2-shot):\")\n",
        "two_shot_caption = generate_caption(model, tokenizer, processed_example_images, example_texts, processed_test_image)\n",
        "print(two_shot_caption)\n",
        "print(\"\\nResults with 4 examples (4-shot):\")\n",
        "four_shot_caption = generate_caption(\n",
        "    model, \n",
        "    tokenizer, \n",
        "    processed_example_images + processed_additional_images, \n",
        "    example_texts + additional_texts, \n",
        "    processed_test_image\n",
        ")\n",
        "print(four_shot_caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we demonstrated the functionality of Open-Flamingo - an open implementation of DeepMind's Flamingo model. We showed:\n",
        "\n",
        "1. How to install and load the Open-Flamingo model\n",
        "2. How to use the model for generating image captions (image captioning)\n",
        "3. How to use the model for answering questions about images (VQA)\n",
        "4. How the quality of results changes with the number of examples (shots)\n",
        "\n",
        "The Flamingo model represents an important step in the development of multimodal models with few-shot learning capabilities, allowing adaptation to new tasks without additional training, using only a few examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Open-Flamingo Demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
